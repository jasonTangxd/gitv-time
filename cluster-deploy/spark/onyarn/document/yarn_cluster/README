Spark Driver首先作为一个ApplicationMaster在YARN集群中启动，
客户端提交给ResourceManager的每一个job都会在集群的worker节点上分配一个唯一的ApplicationMaster，
ApplicationMaster管理全生命周期的应用。因为Driver程序在YARN中运行，所以事先不用启动Spark Master/Client，
应用的运行结果不能在客户端显示（可以在history server中查看），
所以最好将结果保存在HDFS而非stdout输出，客户端的终端显示的是作为YARN的job的简单运行状况。

w1. 由client向ResourceManager提交请求，并上传jar到HDFS上
这期间包括四个步骤：
a). 连接到RM
b). 从RM ASM（ApplicationsManager ）中获得metric、queue和resource等信息。
c). upload app jar and spark-assembly jar
d). 设置运行环境和container上下文（launch-container.sh等脚本)

2. ResouceManager向NodeManager申请资源，创建Spark ApplicationMaster（每个SparkContext都有一个ApplicationMaster）
3. NodeManager启动Spark App Master，并向ResourceManager AsM注册
4. Spark ApplicationMaster从HDFS中找到jar文件，启动DAGscheduler和YARN Cluster Scheduler
5. ResourceManager向ResourceManager AsM注册申请container资源（INFO YarnClientImpl: Submitted application）
6. ResourceManager通知NodeManager分配Container，这时可以收到来自ASM关于container的报告。（每个container的对应一个executor）
7. Spark ApplicationMaster直接和container（executor）进行交互，完成这个分布式任务。

需要注意的是：
a). Spark中的localdir会被yarn.nodemanager.local-dirs替换
b). 允许失败的节点数(spark.yarn.max.worker.failures)为executor数量的两倍数量，最小为3.
c). SPARK_YARN_USER_ENV传递给spark进程的环境变量
d). 传递给app的参数应该通过–args指定。